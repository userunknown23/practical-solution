#CANDIDATE ELIMINATION ALGORITHM
import numpy as np

def is_consistent(hypothesis, instance):
    for i in range(len(hypothesis)):
        if hypothesis[i] != instance[i] and hypothesis[i] != '?':
            return False
    return True

def generalize_specific_bound(hypothesis, instance):
    generalized_hypothesis = list(hypothesis)
    for i in range(len(hypothesis)):
        if hypothesis[i] == '?':
            generalized_hypothesis[i] = instance[i]
        elif hypothesis[i] != instance[i]:
            generalized_hypothesis[i] = '?'
    return generalized_hypothesis

def specialize_general_bound(hypotheses, instance):
    new_hypotheses = []
    for hypothesis in hypotheses:
        for i in range(len(hypothesis)):
            if hypothesis[i] == '?':
                new_hypothesis = list(hypothesis)
                new_hypothesis[i] = instance[i]
                new_hypotheses.append(new_hypothesis)
    return new_hypotheses

def candidate_elimination(training_data):
    num_features = len(training_data[0]) - 1
    specific_hypothesis = ['0'] * num_features
    general_hypotheses = [['?'] * num_features]

    epoch = 0
    hypotheses_history = [(specific_hypothesis, general_hypotheses)]

    while True:
        epoch += 1
        new_specific = specific_hypothesis[:]
        new_general = general_hypotheses[:]

        for instance in training_data:
            x = instance[:-1]
            y = instance[-1]

            if y == '1':  # Positive example
                # Remove inconsistent general hypotheses
                new_general = [h for h in new_general if is_consistent(h, x)]

                # Remove inconsistent specific hypothesis and generalize it
                new_specific = generalize_specific_bound(new_specific, x)

                # Add more specific hypotheses by specializing the general ones
                new_general += specialize_general_bound(new_general, x)

            else:  # Negative example
                # Remove inconsistent specific hypothesis
                if is_consistent(new_specific, x):
                    new_specific = ['?'] * num_features

                # Remove inconsistent general hypotheses and generalize them
                new_general = [h for h in new_general if not is_consistent(h, x)]
                for i in range(len(new_general)):
                    new_general[i] = generalize_specific_bound(new_general[i], x)

        if new_specific == specific_hypothesis and new_general == general_hypotheses:
            break

        hypotheses_history.append((new_specific, new_general))
        specific_hypothesis = new_specific
        general_hypotheses = new_general

    return hypotheses_history

# Example usage
training_data = np.array([
    ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'Y'],
    ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'Y'],
    ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'N'],
    ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'Y']
])

hypotheses_history = candidate_elimination(training_data)

for epoch, (specific, general) in enumerate(hypotheses_history):
    print("Epoch", epoch + 1)
    print("Specific hypothesis:", specific)
    print("General hypotheses:", general)
    print()


#ID3 ALGORITHM
import numpy as np
from collections import Counter

class Node:
    def __init__(self, attribute):
        self.attribute = attribute
        self.children = {}
        self.label = None

def entropy(labels):
    counter = Counter(labels)
    total_instances = len(labels)
    entropy_value = 0.0
    
    for count in counter.values():
        probability = count / total_instances
        entropy_value -= probability * np.log2(probability)
        
    return entropy_value

def choose_best_attribute(data, attributes):
    labels = data[:, -1]
    total_instances = len(labels)
    initial_entropy = entropy(labels)
    best_gain = 0.0
    best_attribute = None
    
    for attribute in attributes:
        attribute_values = np.unique(data[:, attribute])
        attribute_entropy = 0.0
        
        for value in attribute_values:
            subset = data[data[:, attribute] == value]
            subset_labels = subset[:, -1]
            subset_instances = len(subset_labels)
            attribute_entropy += (subset_instances / total_instances) * entropy(subset_labels)
        
        information_gain = initial_entropy - attribute_entropy
        
        if information_gain > best_gain:
            best_gain = information_gain
            best_attribute = attribute
    
    return best_attribute

def create_decision_tree(data, attributes):
    labels = data[:, -1]
    unique_labels = np.unique(labels)
    
    # If all instances have the same label, return a leaf node
    if len(unique_labels) == 1:
        leaf_node = Node(None)
        leaf_node.label = unique_labels[0]
        return leaf_node
    
    # If there are no attributes remaining, return a leaf node with the majority label
    if len(attributes) == 0:
        leaf_node = Node(None)
        leaf_node.label = Counter(labels).most_common(1)[0][0]
        return leaf_node
    
    # Find the best attribute to split the data
    best_attribute = choose_best_attribute(data, attributes)
    
    # Create a new internal node with the best attribute
    root = Node(best_attribute)
    
    # Remove the best attribute from the list of attributes
    attributes = [attr for attr in attributes if attr != best_attribute]
    
    # Split the data based on the best attribute
    attribute_values = np.unique(data[:, best_attribute])
    for value in attribute_values:
        subset = data[data[:, best_attribute] == value]
        
        # If the subset is empty, create a leaf node with the majority label
        if len(subset) == 0:
            leaf_node = Node(None)
            leaf_node.label = Counter(labels).most_common(1)[0][0]
            root.children[value] = leaf_node
        else:
            # Recursively create the subtree
            root.children[value] = create_decision_tree(subset, attributes)
    
    return root

def classify(instance, tree):
    if tree.label is not None:
        return tree.label
    
    attribute = tree.attribute
    value = instance[attribute]
    
    if value not in tree.children:
        return None
    
    child = tree.children[value]
    return classify(instance, child)

# Example usage
training_data = np.array([
    ['Sunny', 'Hot', 'High', 'Weak', 'No'],
    ['Sunny', 'Hot', 'High', 'Strong', 'No'],
    ['Overcast', 'Hot', 'High', 'Weak', 'Yes'],
    ['Rain', 'Mild', 'High', 'Weak', 'Yes'],
    ['Rain', 'Cool', 'Normal', 'Weak', 'Yes'],
    ['Rain', 'Cool', 'Normal', 'Strong', 'No'],
    ['Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],
    ['Sunny', 'Mild', 'High', 'Weak', 'No'],
    ['Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],
    ['Rain', 'Mild', 'Normal', 'Weak', 'Yes'],
    ['Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],
    ['Overcast', 'Mild', 'High', 'Strong', 'Yes'],
    ['Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],
    ['Rain', 'Mild', 'High', 'Strong', 'No']
])

attributes = [0, 1, 2, 3]  # Indices of attributes in the training data

# Create the decision tree
decision_tree = create_decision_tree(training_data, attributes)

# Example instances to classify
test_instance_1 = ['Sunny', 'Cool', 'High', 'Strong']
test_instance_2 = ['Rain', 'Hot', 'Normal', 'Weak']

# Classify the test instances using the decision tree
classification_1 = classify(test_instance_1, decision_tree)
classification_2 = classify(test_instance_2, decision_tree)

# Print the classifications
print("Classification for test instance 1:", classification_1)
print("Classification for test instance 2:", classification_2)


#BACKPROPAGATION ALGORITHM
import numpy as np

X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)

X = X / np.amax(X, axis=0)
y = y / 100

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def derivatives_sigmoid(x):
    return x * (1 - x)

epoch = 5
lr = 0.1
inputlayer_neurons = 2
hiddenlayer_neurons = 3
output_neurons = 1

wh = np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons))
bh = np.random.uniform(size=(1, hiddenlayer_neurons))
wout = np.random.uniform(size=(hiddenlayer_neurons, output_neurons))
bout = np.random.uniform(size=(1, output_neurons))

for i in range(epoch):
    hinp1 = np.dot(X, wh)
    hinp = hinp1 + bh
    hlayer_act = sigmoid(hinp)

    outinp1 = np.dot(hlayer_act, wout)
    outinp = outinp1 + bout
    output = sigmoid(outinp)

    EO = y - output
    outgrad = derivatives_sigmoid(output)
    d_output = EO * outgrad

    EH = d_output.dot(wout.T)
    hiddengrad = derivatives_sigmoid(hlayer_act)
    d_hiddenlayer = EH * hiddengrad

    wout += hlayer_act.T.dot(d_output) * lr
    wh += X.T.dot(d_hiddenlayer) * lr

    print("-----------Epoch-", i + 1, "Starts----------")
    print("Input:\n" + str(X))
    print("Actual Output:\n" + str(y))
    print("Predicted Output:\n", output)
    print("-----------Epoch-", i + 1, "Ends----------\n")

print("Input:\n" + str(X))
print("Actual Output:\n" + str(y))
print("Predicted Output:\n", output)

import matplotlib.pyplot as plt

x = np.array([2, 1, 3])
y = np.array([9, 5, 6])
plt.scatter(x,y)
plt.show()

#NA√èVE BAYESIAN CLASSIFIER
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)
from sklearn import metrics
print("Accuracy(in %):", metrics.accuracy_score(y_test, y_pred)*100)

#NA√èVE BAYESIAN CLASSIFIER
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score

newsgroups_train = fetch_20newsgroups(subset='train')
newsgroups_test = fetch_20newsgroups(subset='test')
vectorizer = CountVectorizer(stop_words='english')
X_train = vectorizer.fit_transform(newsgroups_train.data)
X_test = vectorizer.transform(newsgroups_test.data)

nb = MultinomialNB()
nb.fit(X_train, newsgroups_train.target)
y_pred = nb.predict(X_test)

accuracy = accuracy_score(newsgroups_test.target, y_pred)
precision = precision_score(newsgroups_test.target, y_pred, average='macro')
recall = recall_score(newsgroups_test.target, y_pred, average='macro')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")

import matplotlib.pyplot as plt
accuracy = accuracy_score(newsgroups_test.target, y_pred)
precision = precision_score(newsgroups_test.target, y_pred, average='macro')
recall = recall_score(newsgroups_test.target, y_pred, average='macro')
labels = ['Accuracy', 'Precision', 'Recall']
scores = [accuracy, precision, recall]
plt.bar(labels, scores)
plt.title('Evaluation Metrics')
plt.show()

#BAYESIAN NETWORK
import pandas as pd
import numpy as np
from sklearn.mixture import GaussianMixture
data = pd.read_csv('covid_19_data.csv')
data = data.select_dtypes(include=[np.number])
em = GaussianMixture(n_components=3)
em.fit(data)
labels = em.predict(data)
print(labels)

#EXPECTATION MAXIMIZATION ALGORITHM
from sklearn.cluster import KMeans
from sklearn import preprocessing
from sklearn.mixture import GaussianMixture
from sklearn.datasets import load_iris
import sklearn.metrics as sm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
dataset=load_iris()
X=pd.DataFrame(dataset.data)
X.columns=['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']
y=pd.DataFrame(dataset.target)
y.columns=['Targets']
plt.figure(figsize=(14,7))
colormap=np.array(['red','lime','black'])
plt.subplot(1,3,1)
plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y.Targets],s=40)
plt.title('Real')
plt.subplot(1,3,2)
model=KMeans(n_clusters=3)
model.fit(X)
predY=np.choose(model.labels_,[0,1,2]).astype(np.int64)
plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[predY],s=40)
plt.title('KMeans')
scaler=preprocessing.StandardScaler()
scaler.fit(X)
xsa=scaler.transform(X)
xs=pd.DataFrame(xsa,columns=X.columns)
gmm=GaussianMixture(n_components=3)
gmm.fit(xs)
y_cluster_gmm=gmm.predict(xs)
plt.subplot(1,3,3)
plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y_cluster_gmm],s=40)
plt.title('GMM Classification')

#K-NEAREST NEIGHBOUR ALGORITHM
#???
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split 
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']
dataset = pd.read_csv("8-dataset.csv", names=names)
X = dataset.iloc[:, :-1]
y = dataset.iloc[:, -1]
print(X.head())
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.10)
classifier = KNeighborsClassifier(n_neighbors=5).fit(Xtrain, ytrain)
ypred = classifier.predict(Xtest)
i = 0
print ("\n-------------------------------------------------------------------------")
print ('%-25s %-25s %-25s' % ('Original Label', 'Predicted Label', 'Correct/Wrong'))
print ("-------------------------------------------------------------------------")
for label in ytest:
  print ('%-25s %-25s' % (label, ypred[i]), end="")
  if (label == ypred[i]):
    print (' %-25s' % ('Correct'))
  else:
    print (' %-25s' % ('Wrong'))
i = i + 1
print ("-------------------------------------------------------------------------")
print("\nConfusion Matrix:\n",metrics.confusion_matrix(ytest, ypred))
print ("-------------------------------------------------------------------------")
print("\nClassification Report:\n",metrics.classification_report(ytest, ypred))
print ("-------------------------------------------------------------------------")
print('Accuracy of the classifer is %0.2f' % metrics.accuracy_score(ytest,ypred))
print ("-------------------------------------------------------------------------")

#LOCALLY WEIGHTED REGRESSION ALGORITHM
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
def kernel(point, xmat, k):
    m,n = np.shape(xmat)
    weights = np.mat(np.eye((m)))
    for j in range(m):
        diff = point - X[j]
        weights[j,j] = np.exp(diff*diff.T/(-2.0*k**2))
    return weights
def localWeight(point, xmat, ymat, k):
    wei = kernel(point,xmat,k)
    W = (X.T*(wei*X)).I*(X.T*(wei*ymat.T))
    return W
def localWeightRegression(xmat, ymat, k):
    m,n = np.shape(xmat)
    ypred = np.zeros(m)
    for i in range(m):
        ypred[i] = xmat[i]*localWeight(xmat[i],xmat,ymat,k)
    return ypred
data = pd.read_csv('10-dataset.csv')
bill = np.array(data.total_bill)
tip = np.array(data.tip)
mbill = np.mat(bill)
mtip = np.mat(tip)
m= np.shape(mbill)[1]
one = np.mat(np.ones(m))
X = np.hstack((one.T,mbill.T))
ypred = localWeightRegression(X,mtip,0.5)
SortIndex = X[:,1].argsort(0)
xsort = X[SortIndex][:,0]
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.scatter(bill,tip, color='green')
ax.plot(xsort[:,1],ypred[SortIndex], color = 'red', linewidth=5)
plt.xlabel('Total bill')
plt.ylabel('Tip')
plt.show();